{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69d34a40-7298-48f8-9bba-60fc764d1f20",
   "metadata": {},
   "source": [
    "# Piecing it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e528a983-5a94-4c9f-a942-e32ecca46ede",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import random\n",
    "import boto3\n",
    "import chromadb\n",
    "import streamlit as st\n",
    "from chromadb.utils.embedding_functions.amazon_bedrock_embedding_function import AmazonBedrockEmbeddingFunction\n",
    "from langchain_community.embeddings import BedrockEmbeddings\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "EMBEDDING_MODEL_ID=\"amazon.titan-embed-text-v2:0\"\n",
    "MODELS = {\n",
    "            \"llama3-2-3b\": \"us.meta.llama3-2-3b-instruct-v1:0\",\n",
    "            \"llama3-2-11b\": \"us.meta.llama3-2-11b-instruct-v1:0\",\n",
    "            \"llama3-2-90b\": \"us.meta.llama3-2-90b-instruct-v1:0\",\n",
    "            \"llama3-3-70b\": \"us.meta.llama3-3-70b-instruct-v1:0\"\n",
    "         }\n",
    "\n",
    "\n",
    "session = boto3.Session(profile_name='tap_dev')\n",
    "#session = boto3.Session( region_name=\"us-east-1\")\n",
    "bedrock_rt = session.client(\"bedrock-runtime\",\n",
    "                            region_name=\"us-east-1\",\n",
    "                            )\n",
    "\n",
    "def get_embeddings():\n",
    "\n",
    "    embedding_model_id = \"amazon.titan-embed-text-v2:0\"\n",
    "    return BedrockEmbeddings(client=bedrock_rt, model_id=embedding_model_id)\n",
    "\n",
    "\n",
    "def get_vector_collection() -> chromadb.Collection:\n",
    "\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./brew-rag-bedrock\")\n",
    "    return chroma_client.get_or_create_collection(\n",
    "        name=\"rag_app\",\n",
    "        embedding_function=AmazonBedrockEmbeddingFunction(\n",
    "\n",
    "                model_name=EMBEDDING_MODEL_ID,\n",
    "                session=session\n",
    "            ),\n",
    "        metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "def query_collection(prompt: str, n_results: int = 10):\n",
    "    collection = get_vector_collection()\n",
    "    results = collection.query(query_texts=[prompt], n_results=n_results)\n",
    "    return results\n",
    "\n",
    "\n",
    "def call_llm(context: str, prompt: str, model_name: str = \"llama3-2-3b\", temp: float = 0.5):\n",
    "    system_prompt=f\"\"\"\n",
    "    You are an AI assistant tasked with providing detailed answers based solely on the given context. Your goal is to analyze the information provided and formulate a comprehensive, well-structured response to the question.\n",
    "\n",
    "    context will be passed as within the \"documents\" tags below:\n",
    "    <documents>\n",
    "    {context}\n",
    "    </documents>\n",
    "\n",
    "    Please generate 5 follow up questions based on the given context and add it at the end of the response. Do not generate duplicate blocks of follow up questions.\n",
    "\n",
    "    To answer the question:\n",
    "    1. Thoroughly analyze the context, identifying key information relevant to the question.\n",
    "    2. Organize your thoughts and plan your response to ensure a logical flow of information.\n",
    "    3. Formulate a detailed answer that directly addresses the question, using only the information provided in the context.\n",
    "    4. Ensure your answer is comprehensive, covering all relevant aspects found in the context.\n",
    "    5. If the context doesn't contain sufficient information to fully answer the question, state this clearly in your response.\n",
    "\n",
    "    Format your response as follows:\n",
    "    1. Use clear, concise language.\n",
    "    2. Organize your answer into paragraphs for readability.\n",
    "    3. Use bullet points or numbered lists where appropriate to break down complex information.\n",
    "    4. If relevant, include any headings or subheadings to structure your response.\n",
    "    5. Ensure proper grammar, punctuation, and spelling throughout your answer.\n",
    "\n",
    "    Important: Base your entire response solely on the information provided in the context. Do not include any external knowledge or assumptions not present in the given text.\n",
    "\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"text\": prompt},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    system_pt = [\n",
    "        {\n",
    "            \"text\": system_prompt\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Configuration for the guardrail.\n",
    "    guardrail_config = {\n",
    "        \"guardrailIdentifier\": \"smjijtclxjfc\",\n",
    "        \"guardrailVersion\": \"DRAFT\",\n",
    "        \"trace\": \"enabled\"\n",
    "    }\n",
    "\n",
    "    model_id = MODELS.get(model_name)\n",
    "    streaming_response = bedrock_rt.converse_stream(\n",
    "        modelId=model_id,\n",
    "        messages=messages,\n",
    "        system=system_pt,\n",
    "        inferenceConfig={\"temperature\": temp, \"topP\": 0.9},\n",
    "        guardrailConfig=guardrail_config\n",
    "    )\n",
    "    # Extract and print the streamed response text in real-time.\n",
    "    for chunk in streaming_response[\"stream\"]:\n",
    "        if \"contentBlockDelta\" in chunk:\n",
    "            text = chunk[\"contentBlockDelta\"][\"delta\"][\"text\"]\n",
    "            yield text\n",
    "\n",
    "def re_rank_cross_encoders(documents: list[str]) -> tuple[str, list[int]]:\n",
    "    \"\"\"Re-ranks documents using a cross-encoder model for more accurate relevance scoring.\n",
    "\n",
    "    Uses the MS MARCO MiniLM cross-encoder model to re-rank the input documents based on\n",
    "    their relevance to the query prompt. Returns the concatenated text of the top N most\n",
    "    relevant documents along with their indices.\n",
    "\n",
    "    Args:\n",
    "        documents: List of document strings to be re-ranked.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - relevant_text (str): Concatenated text from the top 3 ranked documents\n",
    "            - relevant_text_ids (list[int]): List of indices for the top ranked documents\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If documents list is empty\n",
    "        RuntimeError: If cross-encoder model fails to load or rank documents\n",
    "    \"\"\"\n",
    "    relevant_text = \"\"\n",
    "    relevant_text_ids = []\n",
    "\n",
    "    encoder_model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "    ranks = encoder_model.rank(prompt, documents, top_k=5)\n",
    "    for rank in ranks:\n",
    "        relevant_text += documents[rank[\"corpus_id\"]]\n",
    "        relevant_text_ids.append(rank[\"corpus_id\"])\n",
    "\n",
    "    return relevant_text, relevant_text_ids\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Document Upload Area\n",
    "    with st.sidebar:\n",
    "        st.set_page_config(page_title=\"Chat with an AI\")\n",
    "        selected_model = st.selectbox(\n",
    "            'Select a model',\n",
    "            options=MODELS.keys(),\n",
    "            index=0\n",
    "            )\n",
    "        temperature = st.slider(\n",
    "            \"Temperature\",\n",
    "            min_value=0.0,\n",
    "            max_value=1.0,\n",
    "            value=0.5)\n",
    "\n",
    "        st.header(\"Don't know what to ask?\")\n",
    "        process = st.button(\n",
    "            \"‚ö°Ô∏è Generate random questions for me\",\n",
    "        )\n",
    "        if process:\n",
    "            collection = get_vector_collection()\n",
    "            all_documents = collection.get()[\"documents\"]\n",
    "            sample_docs = random.sample(all_documents, 5)\n",
    "            prompt=\"Generate 5 questions that can be asked based on the given context\"\n",
    "            response = call_llm(context=sample_docs, prompt=prompt)\n",
    "            st.write(response)\n",
    "\n",
    "    st.header(\"Chat with an AI\")\n",
    "    prompt = st.text_area(\"üó£Ô∏è Ask a question\")\n",
    "    ask = st.button(\n",
    "         \"üî• Ask\",\n",
    "    )\n",
    "\n",
    "    if ask and prompt:\n",
    "        with st.spinner(\"Generating response...\"):\n",
    "            results = query_collection(prompt)\n",
    "            context = results.get(\"documents\")[0]\n",
    "            relevant_text, relevant_text_ids = re_rank_cross_encoders(context)\n",
    "            response = call_llm(context=relevant_text,\n",
    "                            prompt=prompt,\n",
    "                            model_name=selected_model,\n",
    "                            temp=temperature\n",
    "                            )\n",
    "            st.write_stream(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
