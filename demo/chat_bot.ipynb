{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "636407ff-607a-4289-a7d4-0a6d71b29022",
   "metadata": {},
   "source": [
    "### An Example Chat Bot\n",
    "\n",
    "This is an example chatbot. It is half done and has some issues. It sometimes generates unwanted/long output - still debugging. You need to run this app from a terminal. Convert the notebook `.ipynb` file to a `.py` file first using the below command.\n",
    "\n",
    "`jupyter nbconvert --to python chat_bot.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39d8f7a-05fc-4802-beed-6a4395cd30d2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import boto3\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "import streamlit as st\n",
    "import uuid\n",
    "\n",
    "\n",
    "def bedrock_chain():\n",
    "    session = boto3.Session(profile_name='tap_dev')\n",
    "\n",
    "    # Create a bedrock runtime client in us-east-1\n",
    "    bedrock_runtime = session.client(\n",
    "        \"bedrock-runtime\",\n",
    "        region_name=\"us-east-1\"\n",
    "    )\n",
    "\n",
    "    titan_llm = Bedrock(\n",
    "        model_id=\"us.meta.llama3-2-3b-instruct-v1:0\", client=bedrock_runtime, credentials_profile_name=\"tap_dev\", provider=\"meta\"\n",
    "    )\n",
    "\n",
    "    titan_llm.model_kwargs = {\n",
    "        \"temperature\": 0.5,\n",
    "\n",
    "    }\n",
    "\n",
    "    template = \"\"\"System: You are a chatbot that is unhelpful. Your goal is to not help the user but only make jokes.\n",
    "    Take what the user is saying and make a joke out of it.\n",
    "\n",
    "    {chat_history}\n",
    "    \n",
    "    Answer the following human query .\n",
    "    Human: {input}\n",
    "    AI:\"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"chat_history\", \"input\"], \n",
    "        template=template\n",
    "    )\n",
    "    \n",
    "    #memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "    memory = ConversationBufferWindowMemory(memory_key=\"chat_history\", k=2)\n",
    "\n",
    "    conversation = ConversationChain(\n",
    "        prompt=prompt,\n",
    "        llm=titan_llm,\n",
    "        verbose=True,\n",
    "        memory=memory,\n",
    "    )\n",
    "\n",
    "    return conversation\n",
    "\n",
    "def run_chain(chain, prompt):\n",
    "    num_tokens = chain.llm.get_num_tokens(prompt)\n",
    "    return chain({\"input\": prompt}), num_tokens\n",
    "\n",
    "\n",
    "def clear_memory(chain):\n",
    "    return chain.memory.clear()\n",
    "\n",
    "def write_top_bar():\n",
    "    col1, col2, col3 = st.columns([2, 10, 3])\n",
    "    with col2:\n",
    "        header = \"Amazon Bedrock Demo Chatbot\"\n",
    "        st.write(f\"<h3 class='main-header'>{header}</h3>\", unsafe_allow_html=True)\n",
    "    with col3:\n",
    "        clear = st.button(\"Clear Chat\")\n",
    "\n",
    "    return clear\n",
    "\n",
    "\n",
    "def handle_input():\n",
    "    input = st.session_state.input\n",
    "\n",
    "    llm_chain = st.session_state[\"llm_chain\"]\n",
    "    #chain = st.session_state[\"llm_app\"]\n",
    "    result, amount_of_tokens = run_chain(llm_chain, input)\n",
    "    print(result)\n",
    "    question_with_id = {\n",
    "        \"question\": input,\n",
    "        \"id\": len(st.session_state.questions),\n",
    "        \"tokens\": amount_of_tokens,\n",
    "    }\n",
    "    st.session_state.questions.append(question_with_id)\n",
    "\n",
    "    st.session_state.answers.append(\n",
    "        {\"answer\": result, \"id\": len(st.session_state.questions)}\n",
    "    )\n",
    "    st.session_state.input = \"\"\n",
    "\n",
    "\n",
    "def write_user_message(md):\n",
    "    col1, col2 = st.columns([1, 12])\n",
    "\n",
    "    with col1:\n",
    "        st.image(USER_ICON, use_container_width=\"always\")\n",
    "    with col2:\n",
    "        st.warning(md[\"question\"])\n",
    "\n",
    "\n",
    "def render_answer(answer):\n",
    "    col1, col2 = st.columns([1, 12])\n",
    "    with col1:\n",
    "        st.image(AI_ICON, use_container_width=\"always\")\n",
    "    with col2:\n",
    "        st.info(answer[\"response\"])\n",
    "\n",
    "\n",
    "def write_chat_message(md):\n",
    "    chat = st.container()\n",
    "    with chat:\n",
    "        render_answer(md[\"answer\"])\n",
    "\n",
    "\n",
    "USER_ICON = \"https://t3.ftcdn.net/jpg/03/94/89/90/360_F_394899054_4TMgw6eiMYUfozaZU3Kgr5e0LdH4ZrsU.jpg\"\n",
    "AI_ICON = \"https://static.vecteezy.com/system/resources/thumbnails/009/971/218/small/chat-bot-icon-isolated-contour-symbol-illustration-vector.jpg\"\n",
    "\n",
    "if \"user_id\" in st.session_state:\n",
    "    user_id = st.session_state[\"user_id\"]\n",
    "else:\n",
    "    user_id = str(uuid.uuid4())\n",
    "    st.session_state[\"user_id\"] = user_id\n",
    "\n",
    "if \"llm_chain\" not in st.session_state:\n",
    "    st.session_state[\"llm_app\"] = \"bedrock\"\n",
    "    st.session_state[\"llm_chain\"] = bedrock_chain()\n",
    "\n",
    "if \"questions\" not in st.session_state:\n",
    "    st.session_state.questions = []\n",
    "\n",
    "if \"answers\" not in st.session_state:\n",
    "    st.session_state.answers = []\n",
    "\n",
    "if \"input\" not in st.session_state:\n",
    "    st.session_state.input = \"\"\n",
    "\n",
    "clear = write_top_bar()\n",
    "\n",
    "if clear:\n",
    "    st.session_state.questions = []\n",
    "    st.session_state.answers = []\n",
    "    st.session_state.input = \"\"\n",
    "    clear_memory(st.session_state[\"llm_chain\"])\n",
    "\n",
    "\n",
    "with st.container():\n",
    "    for q, a in zip(st.session_state.questions, st.session_state.answers):\n",
    "        write_user_message(q)\n",
    "        write_chat_message(a)\n",
    "\n",
    "\n",
    "st.markdown(\"---\")\n",
    "input = st.text_input(\n",
    "    \"You are talking to an AI, ask any question.\", key=\"input\", on_change=handle_input\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688f9d6d-dd43-41a1-89b5-8e05c9d121d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
